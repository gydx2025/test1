#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°å¢æ¨¡å—æµ‹è¯•è„šæœ¬

æµ‹è¯•æ‰€æœ‰æ–°å¢çš„æ•°æ®å¤„ç†æ¨¡å—æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œ
"""

import logging
import sys
import os
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_data_validator():
    """æµ‹è¯•æ•°æ®éªŒè¯æ¨¡å—"""
    logger.info("=" * 70)
    logger.info("ğŸ§ª æµ‹è¯•æ•°æ®éªŒè¯æ¨¡å— (DataValidator)")
    logger.info("=" * 70)
    
    from data_validator import DataValidator, DataCleaner
    
    validator = DataValidator()
    cleaner = DataCleaner(validator)
    
    # æµ‹è¯•è‚¡ç¥¨ä»£ç éªŒè¯
    logger.info("\nğŸ“‹ æµ‹è¯•è‚¡ç¥¨ä»£ç éªŒè¯:")
    test_codes = ['600000', '000001', '300001', '920000', 'invalid']
    for code in test_codes:
        valid, error = validator.validate_stock_code(code)
        status = "âœ…" if valid else "âŒ"
        logger.info(f"  {status} {code}: {error if error else 'æœ‰æ•ˆ'}")
    
    # æµ‹è¯•å…¬å¸åç§°éªŒè¯
    logger.info("\nğŸ“‹ æµ‹è¯•å…¬å¸åç§°éªŒè¯:")
    test_names = ['ä¸­å›½é“¶è¡Œ', 'ABC', '', 'åç§°<>æ— æ•ˆ']
    for name in test_names:
        valid, error = validator.validate_stock_name(name)
        status = "âœ…" if valid else "âŒ"
        logger.info(f"  {status} '{name}': {error if error else 'æœ‰æ•ˆ'}")
    
    # æµ‹è¯•æ•°æ®æ¸…æ´—
    logger.info("\nğŸ“‹ æµ‹è¯•æ•°æ®æ¸…æ´—:")
    raw_code = 'sh600000'
    clean_code = cleaner.clean_stock_code(raw_code)
    logger.info(f"  æ¸…æ´—å‰: {raw_code}")
    logger.info(f"  æ¸…æ´—å: {clean_code}")
    
    logger.info("\nâœ… æ•°æ®éªŒè¯æ¨¡å—æµ‹è¯•å®Œæˆ\n")


def test_local_storage():
    """æµ‹è¯•æœ¬åœ°å­˜å‚¨æ¨¡å—"""
    logger.info("=" * 70)
    logger.info("ğŸ§ª æµ‹è¯•æœ¬åœ°å­˜å‚¨æ¨¡å— (LocalDatabase)")
    logger.info("=" * 70)
    
    from local_storage import LocalDatabase, CacheManager, CSVBackupManager
    
    # æµ‹è¯•ç¼“å­˜ç®¡ç†
    logger.info("\nğŸ“‹ æµ‹è¯•ç¼“å­˜ç®¡ç†:")
    cache = CacheManager('test_cache')
    
    test_data = {'code': '600000', 'name': 'ä¸­å›½é“¶è¡Œ'}
    cache.save_cache('test', test_data)
    logger.info("  âœ… ç¼“å­˜å·²ä¿å­˜")
    
    loaded = cache.load_cache('test')
    if loaded and loaded.get('code') == '600000':
        logger.info("  âœ… ç¼“å­˜å·²åŠ è½½")
    else:
        logger.error("  âŒ ç¼“å­˜åŠ è½½å¤±è´¥")
    
    # æµ‹è¯•CSVå¤‡ä»½
    logger.info("\nğŸ“‹ æµ‹è¯•CSVå¤‡ä»½:")
    test_records = [
        {'code': '600000', 'name': 'ä¸­å›½é“¶è¡Œ'},
        {'code': '000001', 'name': 'å¹³å®‰é“¶è¡Œ'},
    ]
    
    csv_file = 'test_backup.csv'
    CSVBackupManager.backup_to_csv(test_records, csv_file)
    logger.info(f"  âœ… CSVå·²ä¿å­˜: {csv_file}")
    
    restored = CSVBackupManager.restore_from_csv(csv_file)
    if len(restored) == 2:
        logger.info(f"  âœ… CSVå·²æ¢å¤: {len(restored)}æ¡è®°å½•")
    else:
        logger.error("  âŒ CSVæ¢å¤å¤±è´¥")
    
    # æ¸…ç†
    import shutil
    if os.path.exists('test_cache'):
        shutil.rmtree('test_cache')
    if os.path.exists(csv_file):
        os.remove(csv_file)
    
    logger.info("\nâœ… æœ¬åœ°å­˜å‚¨æ¨¡å—æµ‹è¯•å®Œæˆ\n")


def test_quality_monitor():
    """æµ‹è¯•è´¨é‡ç›‘æ§æ¨¡å—"""
    logger.info("=" * 70)
    logger.info("ğŸ§ª æµ‹è¯•è´¨é‡ç›‘æ§æ¨¡å— (DataQualityMonitor)")
    logger.info("=" * 70)
    
    from quality_monitor import DataQualityScore, DataQualityMonitor
    
    scorer = DataQualityScore()
    monitor = DataQualityMonitor()
    
    # æ¨¡æ‹Ÿæ•°æ®ç»Ÿè®¡
    test_stats = {
        'total_stocks': 5434,
        'valid_records': 5434,
        'total_records': 5434,
        'validation_errors': 0,
        'stocks_with_industry': 5200,
        'stocks_with_2023_data': 4900,
        'stocks_with_2024_data': 5100,
        'collection_date': '2024-12-13'
    }
    
    logger.info("\nğŸ“‹ æµ‹è¯•è´¨é‡è¯„åˆ†è®¡ç®—:")
    quality_report = scorer.calculate_overall_score(test_stats)
    
    logger.info(f"  ç»¼åˆè¯„åˆ†: {quality_report['overall_score']:.1f}/100")
    logger.info(f"  è¯„çº§: {quality_report['grade']}")
    
    for metric, data in quality_report['metrics'].items():
        logger.info(f"  {metric}: {data['score']:.1f}/100")
    
    logger.info("\nğŸ“‹ æµ‹è¯•ç›‘æ§æŠ¥å‘Šç”Ÿæˆ:")
    monitoring_result = monitor.monitor(test_stats)
    report_text = monitor.generate_report(monitoring_result)
    logger.info("\nç”Ÿæˆçš„æŠ¥å‘Šæ‘˜è¦:")
    for line in report_text.split('\n')[0:10]:
        logger.info(f"  {line}")
    
    logger.info("\nâœ… è´¨é‡ç›‘æ§æ¨¡å—æµ‹è¯•å®Œæˆ\n")


def test_checkpoint_manager():
    """æµ‹è¯•æ–­ç‚¹ç»­ä¼ æ¨¡å—"""
    logger.info("=" * 70)
    logger.info("ğŸ§ª æµ‹è¯•æ–­ç‚¹ç»­ä¼ æ¨¡å— (CheckpointManager)")
    logger.info("=" * 70)
    
    from checkpoint_manager import CheckpointManager, VersionManager
    
    checkpoint_mgr = CheckpointManager('test_checkpoints')
    version_mgr = VersionManager('test_version_history.json')
    
    # æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜
    logger.info("\nğŸ“‹ æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜:")
    progress = {'current_page': 5, 'processed': 500}
    checkpoint_mgr.save_checkpoint('test_stage', progress)
    logger.info("  âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜")
    
    # æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½
    logger.info("\nğŸ“‹ æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½:")
    latest = checkpoint_mgr.get_latest_checkpoint('test_stage')
    if latest and latest['progress']['current_page'] == 5:
        logger.info("  âœ… æ£€æŸ¥ç‚¹å·²åŠ è½½")
    else:
        logger.error("  âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥")
    
    # æµ‹è¯•ç‰ˆæœ¬ç®¡ç†
    logger.info("\nğŸ“‹ æµ‹è¯•ç‰ˆæœ¬ç®¡ç†:")
    version_mgr.record_version('v3.0.0', {
        'total_stocks': 5434,
        'data_completeness': 0.98,
        'notes': 'æµ‹è¯•ç‰ˆæœ¬'
    })
    logger.info("  âœ… ç‰ˆæœ¬å·²è®°å½•")
    
    history = version_mgr.get_version_history()
    if len(history) > 0:
        logger.info(f"  âœ… ç‰ˆæœ¬å†å²å·²è·å–: {len(history)}æ¡è®°å½•")
    
    # æ¸…ç†
    import shutil
    if os.path.exists('test_checkpoints'):
        shutil.rmtree('test_checkpoints')
    if os.path.exists('test_version_history.json'):
        os.remove('test_version_history.json')
    
    logger.info("\nâœ… æ–­ç‚¹ç»­ä¼ æ¨¡å—æµ‹è¯•å®Œæˆ\n")


def test_excel_exporter():
    """æµ‹è¯•Excelå¯¼å‡ºæ¨¡å—"""
    logger.info("=" * 70)
    logger.info("ğŸ§ª æµ‹è¯•Excelå¯¼å‡ºæ¨¡å— (ExcelExporter)")
    logger.info("=" * 70)
    
    from excel_exporter import ExcelExporter, ExcelReportGenerator
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_stocks = [
        {'code': '600000', 'name': 'ä¸­å›½é“¶è¡Œ', 'market': 'ä¸Šæµ·', 'list_date': '2001-01-01'},
        {'code': '000001', 'name': 'å¹³å®‰é“¶è¡Œ', 'market': 'æ·±åœ³', 'list_date': '1991-04-03'},
    ]
    
    test_industries = {
        '600000': {'l1': 'é‡‘èä¸š', 'l2': 'é“¶è¡Œä¸š', 'l3': 'å•†ä¸šé“¶è¡Œ', 'source': 'sina'},
        '000001': {'l1': 'é‡‘èä¸š', 'l2': 'é“¶è¡Œä¸š', 'l3': 'å•†ä¸šé“¶è¡Œ', 'source': 'sina'},
    }
    
    test_financial = [
        {
            'code': '600000',
            'name': 'ä¸­å›½é“¶è¡Œ',
            'industry': test_industries['600000'],
            'non_op_real_estate_2023': 1000000,
            'non_op_real_estate_2024': 1200000,
        },
        {
            'code': '000001',
            'name': 'å¹³å®‰é“¶è¡Œ',
            'industry': test_industries['000001'],
            'non_op_real_estate_2023': 500000,
            'non_op_real_estate_2024': 600000,
        },
    ]
    
    test_report = {
        'total_stocks': 2,
        'stocks_with_industry': 2,
        'stocks_with_2023_data': 2,
        'stocks_with_2024_data': 2,
        'data_completeness': 1.0,
        'quality_report': {
            'overall_score': 95,
            'grade': 'Aï¼ˆå¾ˆå¥½ï¼‰'
        }
    }
    
    test_metadata = {
        'collection_date': '2024-12-13',
        'collection_time': datetime.now().isoformat(),
        'version': 'v3.0.0',
        'sources': 'Multi-source',
        'duration': '1.5å°æ—¶',
        'notes': 'æµ‹è¯•æ•°æ®'
    }
    
    # æµ‹è¯•Excelç”Ÿæˆ
    logger.info("\nğŸ“‹ æµ‹è¯•ExcelæŠ¥å‘Šç”Ÿæˆ:")
    output_file = 'test_output.xlsx'
    
    result = ExcelReportGenerator.generate_complete_report(
        stocks=test_stocks,
        industries=test_industries,
        financial_data=test_financial,
        report=test_report,
        metadata=test_metadata,
        filename=output_file
    )
    
    if result and os.path.exists(output_file):
        file_size = os.path.getsize(output_file)
        logger.info(f"  âœ… Excelæ–‡ä»¶å·²ç”Ÿæˆ: {output_file} ({file_size}å­—èŠ‚)")
        os.remove(output_file)
    else:
        logger.error("  âŒ Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥")
    
    logger.info("\nâœ… Excelå¯¼å‡ºæ¨¡å—æµ‹è¯•å®Œæˆ\n")


def test_data_processing_pipeline():
    """æµ‹è¯•æ•°æ®å¤„ç†æµç¨‹æ¨¡å—"""
    logger.info("=" * 70)
    logger.info("ğŸ§ª æµ‹è¯•æ•°æ®å¤„ç†æµç¨‹æ¨¡å— (DataProcessingPipeline)")
    logger.info("=" * 70)
    
    from data_processing_pipeline import DataProcessingPipeline, ProcessingOrchestrator
    
    pipeline = DataProcessingPipeline(enable_local_db=False, enable_checkpoint=False)
    
    # æµ‹è¯•æ•°æ®éªŒè¯
    logger.info("\nğŸ“‹ æµ‹è¯•æ•°æ®éªŒè¯:")
    test_stocks = [
        {'code': '600000', 'name': 'ä¸­å›½é“¶è¡Œ'},
        {'code': '000001', 'name': 'å¹³å®‰é“¶è¡Œ'},
        {'code': 'invalid', 'name': 'æ— æ•ˆè‚¡ç¥¨'},
    ]
    
    valid_stocks, validation_report = pipeline.validate_stocks(test_stocks)
    logger.info(f"  éªŒè¯ç»“æœ: {validation_report['valid']}æœ‰æ•ˆ, {validation_report['invalid']}æ— æ•ˆ")
    
    # æµ‹è¯•æ•°æ®æ¸…æ´—
    logger.info("\nğŸ“‹ æµ‹è¯•æ•°æ®æ¸…æ´—:")
    test_financial = [
        {
            'code': '600000',
            'name': 'ä¸­å›½é“¶è¡Œ',
            'non_op_real_estate_2023': 1000000,
            'non_op_real_estate_2024': 1200000,
        },
    ]
    
    cleaned_data, cleaning_report = pipeline.clean_records(test_financial)
    logger.info(f"  æ¸…æ´—ç»“æœ: è¾“å…¥{cleaning_report['total_input']}, è¾“å‡º{cleaning_report['total_output']}")
    
    # æµ‹è¯•ç»Ÿè®¡ç”Ÿæˆ
    logger.info("\nğŸ“‹ æµ‹è¯•ç»Ÿè®¡ç”Ÿæˆ:")
    stats = pipeline.generate_data_statistics(valid_stocks, cleaned_data, {})
    logger.info(f"  æ€»è‚¡ç¥¨æ•°: {stats['total_stocks']}")
    logger.info(f"  æ€»è®°å½•æ•°: {stats['total_records']}")
    
    logger.info("\nâœ… æ•°æ®å¤„ç†æµç¨‹æ¨¡å—æµ‹è¯•å®Œæˆ\n")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("\n")
    logger.info("=" * 70)
    logger.info("ğŸš€ Aè‚¡æ•°æ®é‡‡é›†ç³»ç»Ÿ v3.0 - æ–°å¢æ¨¡å—æµ‹è¯•")
    logger.info("=" * 70)
    logger.info(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    tests = [
        ("æ•°æ®éªŒè¯æ¨¡å—", test_data_validator),
        ("æœ¬åœ°å­˜å‚¨æ¨¡å—", test_local_storage),
        ("è´¨é‡ç›‘æ§æ¨¡å—", test_quality_monitor),
        ("æ–­ç‚¹ç»­ä¼ æ¨¡å—", test_checkpoint_manager),
        ("Excelå¯¼å‡ºæ¨¡å—", test_excel_exporter),
        ("æ•°æ®å¤„ç†æµç¨‹æ¨¡å—", test_data_processing_pipeline),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            test_func()
            passed += 1
        except Exception as e:
            logger.error(f"âŒ {test_name}æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
    
    # æœ€ç»ˆæ€»ç»“
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š æµ‹è¯•æ€»ç»“")
    logger.info("=" * 70)
    logger.info(f"âœ… é€šè¿‡: {passed}")
    logger.info(f"âŒ å¤±è´¥: {failed}")
    logger.info(f"ğŸ“ˆ æ€»æ•°: {len(tests)}")
    logger.info("=" * 70)
    
    if failed == 0:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        logger.warning(f"âš ï¸ æœ‰{failed}ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == '__main__':
    sys.exit(main())
