#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¤„ç†æµç¨‹ç®¡ç†ç³»ç»Ÿ

é›†æˆæ•°æ®éªŒè¯ã€æ¸…æ´—ã€å­˜å‚¨ã€è´¨é‡ç›‘æ§ç­‰æ‰€æœ‰åŠŸèƒ½
"""

import logging
import os
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import time

from data_validator import DataValidator, DataCleaner, DataDeduplication
from local_storage import LocalDatabase, CacheManager, CSVBackupManager, write_to_local_cache
from quality_monitor import DataQualityScore, DataQualityMonitor
from checkpoint_manager import CheckpointManager, IncrementalUpdate, VersionManager
from excel_exporter import ExcelReportGenerator

logger = logging.getLogger(__name__)


class DataProcessingPipeline:
    """æ•°æ®å¤„ç†æµç¨‹ç®¡ç†ç³»ç»Ÿ"""
    
    def __init__(self, enable_local_db: bool = True, enable_checkpoint: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†æµç¨‹
        
        Args:
            enable_local_db: æ˜¯å¦å¯ç”¨æœ¬åœ°æ•°æ®åº“
            enable_checkpoint: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
        """
        self.validator = DataValidator()
        self.cleaner = DataCleaner(self.validator)
        self.quality_monitor = DataQualityMonitor()
        self.checkpoint_manager = None
        self.local_database = None
        self.cache_manager = CacheManager()
        self.version_manager = VersionManager()
        
        if enable_checkpoint:
            self.checkpoint_manager = CheckpointManager()
        
        if enable_local_db:
            self.local_database = LocalDatabase()
        
        self.validation_stats = {
            'total_records': 0,
            'valid_records': 0,
            'invalid_records': 0,
            'validation_errors': {}
        }
        
        self.start_time = None
        self.end_time = None
    
    def validate_stocks(self, stocks: List[Dict]) -> Tuple[List[Dict], Dict]:
        """
        éªŒè¯è‚¡ç¥¨åˆ—è¡¨
        
        Args:
            stocks: è‚¡ç¥¨åˆ—è¡¨
            
        Returns:
            (æœ‰æ•ˆçš„è‚¡ç¥¨åˆ—è¡¨, éªŒè¯æŠ¥å‘Š)
        """
        valid_stocks = []
        invalid_stocks = []
        validation_report = {
            'total': len(stocks),
            'valid': 0,
            'invalid': 0,
            'errors': []
        }
        
        for stock in stocks:
            valid, error = self.validator.validate_stock_code(stock.get('code'))
            valid2, error2 = self.validator.validate_stock_name(stock.get('name'))
            
            if valid and valid2:
                valid_stocks.append(stock)
                validation_report['valid'] += 1
            else:
                invalid_stocks.append({
                    'stock': stock,
                    'errors': [error, error2] if error and error2 else [error or error2]
                })
                validation_report['invalid'] += 1
                validation_report['errors'].append({
                    'code': stock.get('code'),
                    'error': error or error2
                })
        
        logger.info(f"è‚¡ç¥¨éªŒè¯å®Œæˆ: {len(valid_stocks)}æœ‰æ•ˆ, {len(invalid_stocks)}æ— æ•ˆ")
        
        return valid_stocks, validation_report
    
    def clean_records(self, records: List[Dict]) -> Tuple[List[Dict], Dict]:
        """
        æ¸…æ´—æ•°æ®è®°å½•
        
        Args:
            records: åŸå§‹è®°å½•åˆ—è¡¨
            
        Returns:
            (æ¸…æ´—åçš„è®°å½•, æ¸…æ´—æŠ¥å‘Š)
        """
        self.start_time = time.time()
        
        cleaned_records = []
        cleaning_report = {
            'total_input': len(records),
            'total_output': 0,
            'cleaned': 0,
            'discarded': 0
        }
        
        # é‡ç½®æ¸…æ´—å™¨è®¡æ•°
        self.cleaner.cleaned_count = 0
        self.cleaner.discarded_count = 0
        
        # æ¸…æ´—æ¯æ¡è®°å½•
        for record in records:
            cleaned = self.cleaner.clean_record(record)
            if cleaned:
                # è¿›è¡Œé¢å¤–éªŒè¯
                valid, errors = self.validator.validate_record(cleaned)
                if valid:
                    cleaned_records.append(cleaned)
        
        cleaning_report['total_output'] = len(cleaned_records)
        cleaning_report['cleaned'] = self.cleaner.cleaned_count
        cleaning_report['discarded'] = self.cleaner.discarded_count
        
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: è¾“å…¥{len(records)}, è¾“å‡º{len(cleaned_records)}, "
                   f"æ¸…æ´—{self.cleaner.cleaned_count}, ä¸¢å¼ƒ{self.cleaner.discarded_count}")
        
        return cleaned_records, cleaning_report
    
    def deduplicate_records(self, records: List[Dict]) -> Tuple[List[Dict], Dict]:
        """
        å»é‡å¤„ç†
        
        Args:
            records: è®°å½•åˆ—è¡¨
            
        Returns:
            (å»é‡åçš„è®°å½•, å»é‡æŠ¥å‘Š)
        """
        dedup_report = {
            'before_dedup': len(records),
            'after_dedup': 0,
            'duplicates_removed': 0
        }
        
        # æŒ‰codeå»é‡
        seen_codes = set()
        deduped = []
        
        for record in records:
            code = record.get('code')
            if code and code not in seen_codes:
                seen_codes.add(code)
                deduped.append(record)
        
        dedup_report['after_dedup'] = len(deduped)
        dedup_report['duplicates_removed'] = len(records) - len(deduped)
        
        logger.info(f"å»é‡å®Œæˆ: å»é‡å‰{len(records)}, å»é‡å{len(deduped)}, "
                   f"é‡å¤{dedup_report['duplicates_removed']}")
        
        return deduped, dedup_report
    
    def generate_data_statistics(self, stocks: List[Dict], 
                                 financial_data: List[Dict],
                                 industries: Dict[str, Dict]) -> Dict:
        """
        ç”Ÿæˆæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stocks: è‚¡ç¥¨åˆ—è¡¨
            financial_data: è´¢åŠ¡æ•°æ®
            industries: è¡Œä¸šåˆ†ç±»
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'total_stocks': len(stocks),
            'total_records': len(financial_data),
            'valid_records': len(financial_data),
            'validation_errors': len([r for r in financial_data if not r.get('valid', True)]),
            'stocks_with_industry': len(industries),
            'stocks_with_2023_data': 0,
            'stocks_with_2024_data': 0,
            'code_distribution': {
                '6': 0,  # æ²ª
                '0': 0,  # æ·±
                '3': 0,  # åˆ›ä¸šæ¿
                '8': 0,  # åŒ—äº¤æ‰€
                '4': 0   # å…¶ä»–
            }
        }
        
        # ç»Ÿè®¡2023å’Œ2024å¹´æ•°æ®è¦†ç›–
        for record in financial_data:
            if record.get('non_op_real_estate_2023'):
                stats['stocks_with_2023_data'] += 1
            if record.get('non_op_real_estate_2024'):
                stats['stocks_with_2024_data'] += 1
        
        # ç»Ÿè®¡ä»£ç åˆ†å¸ƒ
        for stock in stocks:
            code = stock.get('code', '')
            if code:
                first_char = code[0]
                if first_char in stats['code_distribution']:
                    stats['code_distribution'][first_char] += 1
        
        # è®¡ç®—è¦†ç›–ç‡
        total = len(stocks)
        if total > 0:
            stats['industry_coverage_rate'] = len(industries) / total
            stats['data_2023_coverage_rate'] = stats['stocks_with_2023_data'] / total
            stats['data_2024_coverage_rate'] = stats['stocks_with_2024_data'] / total
            stats['data_completeness'] = (
                (stats['stocks_with_industry'] + 
                 stats['stocks_with_2023_data'] + 
                 stats['stocks_with_2024_data']) / (total * 3)
            )
        
        stats['collection_date'] = datetime.now().strftime('%Y-%m-%d')
        stats['collection_time'] = datetime.now().isoformat()
        
        return stats
    
    def save_to_local_storage(self, stocks: List[Dict], 
                             industries: Dict[str, Dict],
                             financial_data: List[Dict],
                             version: str = 'v3.0'):
        """
        ä¿å­˜æ•°æ®åˆ°æœ¬åœ°å­˜å‚¨
        
        Args:
            stocks: è‚¡ç¥¨åˆ—è¡¨
            industries: è¡Œä¸šåˆ†ç±»
            financial_data: è´¢åŠ¡æ•°æ®
            version: ç‰ˆæœ¬å·
        """
        if not self.local_database:
            logger.warning("æœ¬åœ°æ•°æ®åº“æœªå¯ç”¨")
            return
        
        try:
            self.local_database.backup_stocks(stocks)
            self.local_database.backup_industries(industries)
            self.local_database.backup_financial_data(financial_data)
            
            # ä¿å­˜ç‰ˆæœ¬ä¿¡æ¯
            version_info = {
                'total_stocks': len(stocks),
                'total_industries': len(industries),
                'stocks_with_2023_data': len([r for r in financial_data if r.get('non_op_real_estate_2023')]),
                'stocks_with_2024_data': len([r for r in financial_data if r.get('non_op_real_estate_2024')]),
                'data_completeness': (len(industries) + 
                                     len([r for r in financial_data if r.get('non_op_real_estate_2023')]) +
                                     len([r for r in financial_data if r.get('non_op_real_estate_2024')])) / 
                                     (len(stocks) * 3) if stocks else 0,
                'notes': 'å®Œæ•´ç‰ˆæœ¬'
            }
            self.local_database.save_version_info(version, version_info)

            # å†™å…¥æœ¬åœ°ç¼“å­˜å±‚ï¼ˆä¾›å¿«é€ŸæŸ¥è¯¢/å‰ç¼€æœç´¢ä½¿ç”¨ï¼‰
            write_to_local_cache(stocks=stocks, industries=industries, version=version)
            
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æœ¬åœ°å­˜å‚¨å¤±è´¥: {e}")
    
    def save_csv_backup(self, financial_data: List[Dict], filename: str):
        """
        ä¿å­˜CSVå¤‡ä»½
        
        Args:
            financial_data: è´¢åŠ¡æ•°æ®
            filename: æ–‡ä»¶å
        """
        CSVBackupManager.backup_to_csv(financial_data, filename)
    
    def generate_quality_report(self, stats: Dict) -> Dict:
        """
        ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        
        Args:
            stats: æ•°æ®ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            è´¨é‡æŠ¥å‘Š
        """
        monitoring_result = self.quality_monitor.monitor(stats)
        return monitoring_result
    
    def generate_final_report(self, stats: Dict, quality_report: Dict) -> Dict:
        """
        ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        
        Args:
            stats: æ•°æ®ç»Ÿè®¡
            quality_report: è´¨é‡æŠ¥å‘Š
            
        Returns:
            æœ€ç»ˆæŠ¥å‘Š
        """
        if self.start_time and not self.end_time:
            self.end_time = time.time()
        
        duration = self.end_time - self.start_time if self.start_time and self.end_time else 0
        
        final_report = {
            'timestamp': datetime.now().isoformat(),
            'duration_seconds': duration,
            'duration_minutes': duration / 60,
            'statistics': stats,
            'quality_report': quality_report,
            'version': '3.0.0 - å®Œæ•´ç‰ˆ',
            'status': 'SUCCESS' if stats.get('data_completeness', 0) >= 0.98 else 'WARNING'
        }
        
        return final_report
    
    def close(self):
        """å…³é—­æ‰€æœ‰èµ„æº"""
        if self.local_database:
            self.local_database.close()
        logger.info("æ•°æ®å¤„ç†æµç¨‹å·²å…³é—­")


class ProcessingOrchestrator:
    """æ•°æ®å¤„ç†æµç¨‹ç¼–æ’å™¨ - ç®¡ç†æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµç¨‹ç¼–æ’å™¨"""
        self.pipeline = DataProcessingPipeline()
    
    def process_complete_pipeline(
        self,
        stocks: List[Dict],
        industries: Dict[str, Dict],
        financial_data: List[Dict],
        output_filename: str = None,
        csv_backup_filename: str = None
    ) -> Tuple[List[Dict], Dict, str]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
        
        Args:
            stocks: åŸå§‹è‚¡ç¥¨åˆ—è¡¨
            industries: åŸå§‹è¡Œä¸šåˆ†ç±»
            financial_data: åŸå§‹è´¢åŠ¡æ•°æ®
            output_filename: Excelè¾“å‡ºæ–‡ä»¶å
            csv_backup_filename: CSVå¤‡ä»½æ–‡ä»¶å
            
        Returns:
            (æ¸…æ´—åçš„è´¢åŠ¡æ•°æ®, æœ€ç»ˆæŠ¥å‘Š, è¾“å‡ºæ–‡ä»¶è·¯å¾„)
        """
        logger.info("=" * 70)
        logger.info("ğŸ”„ å¼€å§‹æ•°æ®å¤„ç†æµç¨‹")
        logger.info("=" * 70)
        
        # æ­¥éª¤1: éªŒè¯è‚¡ç¥¨åˆ—è¡¨
        logger.info("\n[æ­¥éª¤1] éªŒè¯è‚¡ç¥¨åˆ—è¡¨...")
        valid_stocks, stock_validation = self.pipeline.validate_stocks(stocks)
        
        # æ­¥éª¤2: æ¸…æ´—è´¢åŠ¡æ•°æ®
        logger.info("\n[æ­¥éª¤2] æ¸…æ´—è´¢åŠ¡æ•°æ®...")
        cleaned_data, cleaning_report = self.pipeline.clean_records(financial_data)
        
        # æ­¥éª¤3: å»é‡å¤„ç†
        logger.info("\n[æ­¥éª¤3] å»é‡å¤„ç†...")
        deduped_data, dedup_report = self.pipeline.deduplicate_records(cleaned_data)
        
        # æ­¥éª¤4: ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        logger.info("\n[æ­¥éª¤4] ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
        stats = self.pipeline.generate_data_statistics(valid_stocks, deduped_data, industries)
        
        # æ­¥éª¤5: ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        logger.info("\n[æ­¥éª¤5] ç”Ÿæˆè´¨é‡æŠ¥å‘Š...")
        quality_report = self.pipeline.generate_quality_report(stats)
        
        # æ­¥éª¤6: ä¿å­˜æœ¬åœ°å­˜å‚¨
        logger.info("\n[æ­¥éª¤6] ä¿å­˜æœ¬åœ°å­˜å‚¨...")
        self.pipeline.save_to_local_storage(valid_stocks, industries, deduped_data)
        
        # æ­¥éª¤7: ä¿å­˜CSVå¤‡ä»½
        if csv_backup_filename:
            logger.info("\n[æ­¥éª¤7] ä¿å­˜CSVå¤‡ä»½...")
            self.pipeline.save_csv_backup(deduped_data, csv_backup_filename)
        
        # æ­¥éª¤8: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        logger.info("\n[æ­¥éª¤8] ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        final_report = self.pipeline.generate_final_report(stats, quality_report)
        
        # æ­¥éª¤9: ç”ŸæˆExcelæŠ¥å‘Š
        logger.info("\n[æ­¥éª¤9] ç”ŸæˆExcelæŠ¥å‘Š...")
        if not output_filename:
            output_filename = f"Aè‚¡éç»è¥æ€§æˆ¿åœ°äº§èµ„äº§_å®Œæ•´ç‰ˆ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        metadata = {
            'collection_date': stats.get('collection_date'),
            'collection_time': stats.get('collection_time'),
            'version': final_report.get('version'),
            'sources': 'Multi-source aggregation',
            'duration': f"{final_report.get('duration_minutes', 0):.1f}åˆ†é’Ÿ",
            'file_size': '',
            'notes': f"æ€»è®¡{stats.get('total_stocks')}åªè‚¡ç¥¨ï¼Œæ•°æ®å®Œæ•´åº¦{stats.get('data_completeness', 0)*100:.1f}%"
        }
        
        excel_file = ExcelReportGenerator.generate_complete_report(
            valid_stocks,
            industries,
            deduped_data,
            {**stats, 'quality_report': quality_report},
            metadata,
            output_filename
        )
        
        # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š æ•°æ®å¤„ç†å®Œæˆæ€»ç»“")
        logger.info("=" * 70)
        logger.info(f"â±ï¸ å¤„ç†ç”¨æ—¶: {final_report['duration_minutes']:.1f}åˆ†é’Ÿ ({final_report['duration_seconds']:.0f}ç§’)")
        logger.info(f"ğŸ“ˆ å¤„ç†è‚¡ç¥¨: {stats['total_stocks']}åª")
        logger.info(f"âœ… æœ‰æ•ˆæ•°æ®: {len(deduped_data)}æ¡")
        logger.info(f"ğŸ“Š æ•°æ®å®Œæ•´åº¦: {stats['data_completeness']*100:.1f}%")
        logger.info(f"â­ è´¨é‡è¯„åˆ†: {quality_report['quality_score']['overall_score']:.1f}/100 [{quality_report['quality_score']['grade']}]")
        logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {excel_file}")
        logger.info("=" * 70)
        
        # æ‰“å°è´¨é‡æŠ¥å‘Š
        quality_report_text = self.pipeline.quality_monitor.generate_report(quality_report)
        logger.info("\n" + quality_report_text)
        
        self.pipeline.close()
        
        return deduped_data, final_report, excel_file
